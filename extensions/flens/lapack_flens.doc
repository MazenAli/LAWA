@Key:       getrf
@Name:      trf
@Short:     compute an $LU$ factorization  of  a  general  $M \times N$
            matrix $A$ using partial pivoting with row interchanges
@Purpose:   computes an $LU$  factorization  of  a  general  $M \times N$
            matrix  $A$  using partial pivoting with row interchanges. The
            factorization has the form
                $A = P L U$
            where $P$ is a permutation matrix, $L$ is lower triangular  with
            unit  diagonal  elements (lower trapezoidal if $m > n$), and $U$
            is upper triangular (upper trapezoidal if $m < n$).
@Arguments:
[A]         (input/output)
            On  entry,  the  $M \times N$  matrix  to be factored.  On
            exit, the factors $L$ and $U$ from the factorization
            $A = P L U$;  the  unit  diagonal  elements  of  $L$ are not
            stored.
[P]         (output)
            The pivot indices; for $1 \leq i \leq \min\{M,N\}$, row $i$  of
            the matrix was interchanged with row $P(i)$.
@Returns:
[$i=0$]     successful exit
[$i>0$]     then $U(i,i)$ is exactly zero.  The factorization  has  been
            completed, but the factor $U$ is exactly singular, and division
            by zero will occur if it is used to solve a system of equations.

@@

@Key:       getri
@Name:      tri
@Short:     compute the inverse of a matrix using the $LU$ factorization
            computed by <<trf (getrf)>>
@Purpose:   <<tri (getri)>> computes the inverse of a matrix using the LU
            factorization  computed  by <<trf (getrf)>>. This function inverts
            $U$ and then computes $A^{-1}$ by solving the system
            $A^{-1}*L = U^{-1}$ for $A^{-1}$.
@Arguments:
[A]         (input/output)
            On entry, the factors $L$ and $U$ from the factorization
            $A = PLU$ as computed by <<trf (getrf)>>.  On successful exit,
            the inverse of the original matrix $A$.
[P]         (input)
            The pivot indices from <<trf (getrf)>>; for $1 \leq i \leq N$,
            row $i$ of the matrix was interchanged with row $P(i)$.
@Returns:
[$i=0$]     successful exit
[$i>0$]     then $U(i,i)$ is exactly zero; the matrix is singular and its inverse
            could not be computed.

@@

@Key:       gbtrf
@Name:      trf
@Short:     compute an $LU$ factorization of a real  $M \times N$ band matrix
            $A$ using partial pivoting with row interchanges 
@Purpose:   <<trf (gbtrf)>> computes an $LU$ factorization of a real $M \times N$
            band matrix $A$ using partial pivoting with row interchanges. This
            is the blocked version of the algorithm, calling Level 3 BLAS.
@Arguments:
[A]         (input/output)
            On entry, the matrix $A$ in band storage and on exit overwritten with
            the $LU$ factorization.  Matrix {\tt A} is required to have a total of
            $k_l$ subdiagonals and $k_l + k_u$ superdiagonals where on entry only
            the elements within the $k_l$ subdiagonals and $k_u$ superdiagonals need
            to be set (See Further Details).
[P]         (input)
            The pivot indices; for $1 \leq i \leq N$,
            row $i$ of the matrix was interchanged with row $P(i)$.
@Returns:   
[$i=0$]     successful exit
[$i>0$]     then $U(i,i)$ is exactly zero.  The factorization  has  been
            completed, but the factor $U$ is exactly singular, and division
            by zero will occur if it is used to solve a system of equations.
@Details:
            Assume non-zero elements of $A$ reside within a band of $k_l$ subdiagonals
            and $k_u$ superdiagonals. Then storing its $LU$ factorization
            requires a band of $k_l$ subdiagonals and $k_l+k_u$ superdiagonals. Hence,
            the {\tt GbMatrix} holding the matrix $A$ needs to have $k_l$ subdiagonals
            and $k_l+k_u$ superdiagonals allocated; but only the elements within its
            $k_l$ subdiagonals and $k_u$ superdiagonals must be set.
            
            If, for example, $A$ is a tridiagonal matrix (i.\,e.\ $k_l = k_u = 1$),
            then matrix $U$ will have in general two super-diagonals.  This requires
            the {\tt GbMatrix} storing $A$ has allocated an additional superdiagonal
            (as indicated by `$*$'):
            \begin{center}
            \begin{tabular}{ccc}
            $ A = \begin{pmatrix}
                a_{1,1} & a_{1,2} & *       & 0       & 0       \\
                a_{2,1} & a_{2,2} & a_{2,3} & *       & 0       \\
                0       & a_{3,2} & a_{3,3} & a_{3,4} & *       \\
                0       & 0       & a_{4,3} & a_{4,4} & a_{4,5} \\    
                0       & 0       & 0       & a_{5,4} & a_{5,5} \\    
            \end{pmatrix} $
            &
            $ \leadsto$ 
            &
            $ A_{LU} =
            \begin{pmatrix}
            u_{1,1} & u_{1,2} & u_{1,3} & 0       & 0       \\
            m_{2,1} & u_{2,2} & u_{2,3} & u_{2,4} & 0       \\
            0       & m_{3,2} & u_{3,3} & u_{3,4} & u_{3,5} \\
            0       & 0       & m_{4,3} & u_{4,4} & u_{4,5} \\  
            0       & 0       & 0       & m_{5,4} & u_{5,5} \\    
            \end{pmatrix}$. \\
            \end{tabular}
            \end{center}
            Elements of $L$ are stored (in general rearranged due to pivoting) on
            the sub-diagonal of $A_{LU}$.
@@

@Key:       getrs
@Name:      trs
@Short:     solve a system of linear equations $A X = B$ or  $A^T X = B$
            with a general $N \times N$ matrix $A$ using the $LU$ factorization
            computed by <<trf (getrf)>>
@Purpose:   <<trs (getrs)>> solves a system of linear equations $A X = B$ or
            $A^T X = B$ with a general $N \times N$ matrix $A$ using the $LU$
            factorization computed by <<trf (getrf)>>.
@Arguments:
[trans]     (input)
            Specifies the form of the system of equations:
            
            \begin{tabular}{ll}
            {\tt trans = NoTrans}&    $A   X = B$  (No transpose)\\
            {\tt trans = Trans}&      $A^T X = B$  (Transpose)\\
            {\tt trans = ConjTrans}&  $A^H X = B$  (Conjugate transpose = Transpose)
            \end{tabular}
[A]         (input)
            The factors $L$ and $U$ from the factorization $A = PLU$
            as computed by <<trf (getrf)>>.
[P]         (input)
            The pivot indices from <<trf (getrf)>>; for $1 \leq i \leq N$,
            row $i$ of the matrix was interchanged with row $P(i)$.
[B]         (input/output)
            On  entry,  the  right hand side matrix $B$.  On exit,
            the solution matrix $X$.
@Returns:
[$i=0$]     successful exit

@@

@Key:       gbtrs
@Name:      trs
@Short:     solve a system of linear equations $A  X = B$ or  $A^T X = B$ with
            a general band matrix $A$ using the LU factorization computed by
            <<trf (gbtrf)>>
@Purpose:   <<trs (gbtrs)>> solves a system of linear equations $A X = B$ or
            $A^T X = B$ with a general band matrix $A$ using the $LU$ factorization
            computed by <<trf (gbtrf)>>.
@Arguments:
[trans]     (input)
            Specifies the form of the system of equations:
            
            \begin{tabular}{ll}
            {\tt trans = NoTrans}&    $A   X = B$  (No transpose)\\
            {\tt trans = Trans}&      $A^T X = B$  (Transpose)\\
            {\tt trans = ConjTrans}&  $A^H X = B$  (Conjugate transpose = Transpose)
            \end{tabular}
[A]         (input)
            Details of the $LU$ factorization of the band matrix $A$,  as  computed
            by <<trf (gbtrf)>>.  $U$ is stored as an upper triangular band matrix
            in the diagonal and the $k_l + k_u$ superdiagonals of $A$; the
            multipliers (i.\,e.\ the elements of $L$ rearranged due to pivoting)
            used during the factorization are  stored in the $k_l$ subdiagonals of
            $A$.
[P]         (input)
            The pivot indices from <<trf (getrf)>>; for $1 \leq i \leq N$,
            row $i$ of the matrix was interchanged with row $P(i)$.
[B]         (input/output)
            On  entry,  the  right hand side matrix $B$.  On exit,
            the solution matrix $X$.
@Returns:
            [$i=0$]     successful exit

@@

@Key:       gesv
@Name:      sv
@Short:     compute the solution to  a  real  system  of  linear
            equations $A X = B$
@Purpose:   computes the solution to a real or complex  system  of  linear
            equations $A X = B$, where $A$ is an $N \times N$ matrix and
            $X$ and $B$ are $N \times R$ matrices.

            The $LU$ decomposition with partial pivoting  and  row
            interchanges is used to factor $A$ as
                \[A = P L U,\]
            where $P$ is a permutation matrix, $L$ is unit lower triangular,
            and  $U$  is upper triangular.  The factored form of $A$ is then
            used to solve the system of equations $A X = B$.
@Arguments:
[A]         (input/output)
            On entry, the $N \times N$ coefficient matrix $A$.  On exit,
            the  factors  $L$  and  $U$  from  the factorization
            $A = P L U$; the unit  diagonal  elements  of  $L$  are  not
            stored.
[P]         (output)
            The pivot indices; for $1 \leq i \leq \min\{M,N\}$, row $i$  of
            the matrix was interchanged with row $P(i)$.
[B]         (input/output)
            On  entry,  the  $N \times R$ matrix of right hand side
            matrix $B$.  On exit, if function returned 0, the
            $N \times R$ solution matrix $X$.
@Returns:
[$i=0$]     successful exit
[$i>0$]     then $U(i,i)$ is exactly zero.  The factorization  has  been
            completed, but the factor $U$ is exactly singular, so the solution
            could not be computed.

@@

@Key:       gbsv
@Name:      sv
@Short:     compute the solution to a real system of linear equations
            $A X  = B$, where $A$ is a band matrix of order $N$ with
            $k_l$ subdiagonals and $k_u$ superdiagonals, and $X$ and $B$
            are $N \times R$ matrices
@Purpose:   <<sv (gbsv)>> computes the solution to a real system of linear
            equations $A X = B$, where $A$ is a band matrix of order $N$ with
            $k_l$ subdiagonals and $k_u$ superdiagonals, and $X$ and  $B$  are
            $N \times R$ matrices.  The $LU$ decomposition with partial pivoting
            and row interchanges is used to factor $A$ as $A$ = $L U$, where
            $L$ is a product of permutation and unit lower triangular matrices with
            $k_l$ subdiagonals, and  $U$ is upper triangular with  $k_l+k_u$
            superdiagonals.  The factored form of $A$ is then used to solve the
            system of equations $A X = B$.
@Arguments:
[A]         (input/output)
            On entry, the matrix $A$ in band storage and on exit overwritten with
            the $LU$ factorization.  Matrix {\tt A} is required to have a total of
            $k_l$ subdiagonals and $k_l + k_u$ superdiagonals where on entry only
            the elements within the $k_l$ subdiagonals and $k_u$ superdiagonals need
            to be set (See Further Details).
[P]         (input)
            The pivot indices; for $1 \leq i \leq N$,
            row $i$ of the matrix was interchanged with row $P(i)$.
[B]         (input/output)
            On  entry,  the  right hand side matrix $B$.  On exit,
            the solution matrix $X$.
@Returns:
[$i=0$]     successful exit
[$i>0$]     then $U(i,i)$ is exactly zero.  The factorization  has  been
            completed, but the factor $U$ is exactly singular, and division
            by zero will occur if it is used to solve a system of equations.
@Details:
            Assume non-zero elements of $A$ reside within a band of $k_l$ subdiagonals
            and $k_u$ superdiagonals. Then storing its $LU$ factorization
            requires a band of $k_l$ subdiagonals and $k_l+k_u$ superdiagonals. Hence,
            the {\tt GbMatrix} holding the matrix $A$ needs to have $k_l$ subdiagonals
            and $k_l+k_u$ superdiagonals allocated; but only the elements within its
            $k_l$ subdiagonals and $k_u$ superdiagonals must be set.
            
            If, for example, $A$ is a tridiagonal matrix (i.\,e.\ $k_l = k_u = 1$),
            then matrix $U$ will have in general two super-diagonals.  This requires
            the {\tt GbMatrix} storing $A$ has allocated an additional superdiagonal
            (as indicated by `$*$'):
            \begin{center}
            \begin{tabular}{ccc}
            $ A = \begin{pmatrix}
                a_{1,1} & a_{1,2} & *       & 0       & 0       \\
                a_{2,1} & a_{2,2} & a_{2,3} & *       & 0       \\
                0       & a_{3,2} & a_{3,3} & a_{3,4} & *       \\
                0       & 0       & a_{4,3} & a_{4,4} & a_{4,5} \\    
                0       & 0       & 0       & a_{5,4} & a_{5,5} \\    
            \end{pmatrix} $
            &
            $ \leadsto$ 
            &
            $ A_{LU} =
            \begin{pmatrix}
            u_{1,1} & u_{1,2} & u_{1,3} & 0       & 0       \\
            m_{2,1} & u_{2,2} & u_{2,3} & u_{2,4} & 0       \\
            0       & m_{3,2} & u_{3,3} & u_{3,4} & u_{3,5} \\
            0       & 0       & m_{4,3} & u_{4,4} & u_{4,5} \\  
            0       & 0       & 0       & m_{5,4} & u_{5,5} \\    
            \end{pmatrix}$. \\
            \end{tabular}
            \end{center}
            Elements of $L$ are stored (in general rearranged due to pivoting) on
            the sub-diagonal of $A_{LU}$.

@@

@Key:       trtrs
@Name:      trs
@Short:     solve a triangular system of the form  $A X = B$ or $A^T X = B$
@Purpose:   <<trs (trtrs)>> solves a triangular system of the form $A X =  B$  or
            $A^T X = B$, where $A$ is a triangular matrix of order $N$, and
            $B$ is an $N \times R$ matrix.  A check is made to verify that  $A$
            is nonsingular.
@Arguments:
[trans]     (input)
            Specifies the form of the system of equations:
            
            \begin{tabular}{ll}
            {\tt trans = NoTrans}&    $A   X = B$  (No transpose)\\
            {\tt trans = Trans}&      $A^T X = B$  (Transpose)\\
            {\tt trans = ConjTrans}&  $A^H X = B$  (Conjugate transpose = Transpose)
            \end{tabular}
[A]         (input)
            The (unit or non-unit) triangular matrix A.
[B]         (input/output)
            On  entry,  the  $N \times R$ matrix of right hand side
            matrix $B$.  On exit, if function returned 0, the
            $N \times R$ solution matrix $X$.
@Returns:
[$i=0$]     successful exit
[$i>0$]     then $A(i,i)$ is zero, indicating that the matrix is singular and the
            solutions $X$ have not been computed.

@@

@Key:       geqrf
@Name:      qrf
@Short:     compute a $QR$ factorization of a real $M \times N$ matrix $A$
@Purpose:   <<qrf (geqrf)>> computes a $QR$ factorization of a real 
            $M \times N$ matrix $A$:  \[ A = Q R. \]
@Arguments:
[A]         (input/output)
            On  entry,  the  $M \times N$ matrix $A$.  On exit, the elements
            on and above the diagonal of the array contain the  
            $\min\{M,N\} \times N$  upper trapezoidal matrix $R$ ($R$ is
            upper triangular if $m \geq n$); the elements below  the
            diagonal, with the array {\tt tau}, represent the orthogonal
            matrix $Q$ as a  product  of  $\min\{m,n\}$  elementary reflectors
            (see Further Details).
[tau]       (output)
            The scalar factors $\tau$ of the elementary reflectors (see
            Further Details).
@Returns:
            [$i=0$]     successful exit
@Details:   The matrix $Q$ is  represented  as  a  product  of  elementary
            reflectors
            \[
            Q = H_1 H_2 \cdots H_k, \;\text{where}\; k = \min\{m,n\}.
            \]
            Each $H_i$ has the form
            \[
            H_i = I - \tau * v * v'
            \]
            where $\tau$ is a real scalar, and $v$ is a real vector with
            $v_1 = \dots v_{i-1} = 0$ and $v_i = 1$;
            $(v_{i+1}, \dots, v_{m})$ is  stored  on  exit  in
            {\tt A(i+1,i), .., A(m,i)} and $\tau$ in {\tt tau(i)}.

@@

@Key:       orgqr
@Name:      orgqr
@Short:     generate an $M \times N$ real matrix $Q$  with  orthonormal columns
@Purpose:   <<orgqr (orgqr)>> generates an $M \times N$ real matrix  $Q$  with
            orthonormal columns,  which  is defined as the first $N$ columns of
            a product of $k$ elementary reflectors of order $M$
            \[
                Q  =  H_1 H_2 \cdot \dots \cdot H_k
            \]
            as returned by <<qrf (geqrf)>>.
@Arguments:
[A]         (input/output)
            On  entry,  the  $i$-th column must contain the vector which defines
            the elementary reflector $H_i$, for $i = 1,2, \dots,k$, as returned
            by <<qrf (geqrf)>> in the first $k$ columns of its matrix argument $A$.
            On exit, the $M \times N$ matrix Q.
[tau]       (input)
            {\tt TAU(i)} must contain the scalar factor of the elementary reflector
            $H_i$, as returned by <<qrf (geqrf)>>.
@Returns:
[$i=0$]     successful exit

@@

@Key:       ormqr
@Name:      ormqr
@Short:     
@Purpose:   <<ormqr (ormqr)>> overwrites the general real $M \times N$ matrix $C$
            as follows:
            \[  C \leftarrow
                \begin{cases}
                Q C   & \text{if {\tt side=Left} and {\tt trans=NoTrans}} \\
                Q^T C & \text{if {\tt side=Left} and {\tt trans=Trans}} \\
                C Q   & \text{if {\tt side=Right} and {\tt trans=NoTrans}} \\
                C Q^T & \text{if {\tt side=Right} and {\tt trans=Trans}}
                \end{cases}
            \]
            where $Q$ is a real orthogonal matrix defined as  the  product
            of $k$ elementary reflectors
            \[
                Q  =  H_1 H_2 \cdot \dots \cdot H_k
            \]
            as returned by <<qrf (geqrf)>>. $Q$ is of order $M$ if {\tt side=Left}
            and  of order $N$ if {\tt side=Right}.
@Arguments:
[side]      (input)
            \begin{tabular}{ll}
            {\tt side = Left}  & apply $Q$ or $Q^T$ from left\\
            {\tt side = Right} & apply $Q$ or $Q^T$ from right
            \end{tabular}
[trans]     (input)
            \begin{tabular}{ll}
            {\tt trans=NoTrans}  & No transpose, apply $Q$\\
            {\tt trans=Trans}    & Transpose, apply $Q^T$
            \end{tabular}
[A]         (input)
            On  entry,  the  $i$-th column must contain the vector which defines
            the elementary reflector $H_i$, for $i = 1,2, \dots,k$, as returned
            by <<qrf (geqrf)>> in the first $k$ columns of its matrix argument $A$.
            $A$ is modified by the routine but restored on exit.
[tau]       (input)
            {\tt TAU(i)} must contain the scalar factor of the elementary reflector
            $H_i$, as returned by <<qrf (geqrf)>>.
[C]         (input/output)
            On  entry,  the  $M \times N$ matrix $C$.
            
            On  exit, $C$ is overwritten by $Q C$ or $Q^T C$ or $C Q^T$ or $C Q$.
@Returns:
[$i=0$]     successful exit

@@

@Key:       gels
@Name:      ls
@Short:     solve overdetermined or underdetermined real linear systems
            involving  an  $M \times N$  matrix  $A$, or its transpose,
            using a $QR$ or $LQ$ factorization of $A$
@Purpose:   <<ls (gels)>> solves overdetermined or underdetermined  real
            linear systems  involving  an  $M \times N$  matrix  $A$, or
            its transpose, using a $QR$ or $LQ$ factorization of $A$. It
            is  assumed  that $A$ has full rank. The following options are
            provided:
            \begin{enumerate}
            \item If {\tt trans = NoTrans} and $M \geq N$: find the least
                  squares solution of an overdetermined system, i.\,e.\,,
                  solve the  least  squares problem
                  \[
                        || B - AX || \to \text{min}.
                  \]

            \item If {\tt trans = NoTrans} and $M < N$:  find the minimum
                  norm solution of an underdetermined system $A  X = B$.

            \item If {\tt trans = Trans} and $M \geq N$:  find the minimum
                  norm  solution of an undetermined system $A^T  X = B$.

            \item If {\tt trans = Trans} and $M < N$:  find the least  squares
                  solution of an overdetermined system, i.\,e.\,, solve the
                  least  squares problem
                  \[
                        || B - A^T * X ||  \to \text{min}.
                  \]
            \end{enumerate}
            Several right hand side vectors $b$ and solution vectors $x$
            can be  handled in a single call; they are stored as the
            columns of the $M \times R$ right hand side matrix $B$ and
            the $N \times R$ solution matrix $X$.
@Arguments:
[trans]     (input)
            \begin{tabular}{ll}
            {\tt trans = NoTrans}&    the linear system involves $A$;\\
            {\tt trans = Trans}&      the linear system involves $A^T$.
            \end{tabular}
[A]         (input/output)
            On  entry, the $M \times N$ matrix $A$.  On exit, if $M \geq N$,
            $A$ is overwritten by details of its $QR$ factorization as
            returned by <<qrf (geqrf)>>; if $M <  N$, $A$ is overwritten
            by details of its $LQ$ factorization  as  returned  by
            <<lqf (gelqf)>>.
[B]         (input/output)
            On  entry,  the matrix $B$ of right hand side vectors,
            stored column-wise; $B$ is $M \times R$ if
            {\tt trans=NoTrans}, or $N \times R$ if {\tt trans=Trans}.
            
            On exit, $B$ is overwritten by the solution vectors,
            stored  column-wise:
            \begin{enumerate}
            \item if {\tt trans=NoTrans} and $M \geq N$, rows $1$ to $N$
                  of $B$ contain the least squares solution vectors; the
                  residual sum  of squares for the solution in each column
                  is given by the sum of squares of elements  $N+1$  to  $M$
                  in  that column;
            \item if {\tt trans=NoTrans} and $M < N$, rows $1$ to $N$ of $B$
                  contain the minimum norm solution vectors;
            \item if  {\tt trans=Trans} and  $M \geq N$,  rows $1$ to $M$
                  of $B$ contain the minimum norm solution vectors;
            \item if {\tt trans=Trans} and  $M < N$, rows  $1$ to $M$ of $B$
                  contain the least squares solution vectors; the residual sum
                  of  squares for the  solution  in each column is given by the
                  sum of squares of elements $M+1$ to $N$ in that column.
            \end{enumerate}
@Returns:
[$i=0$]     successful exit

@@

@Key:       gelss
@Name:      lss
@Short:     compute the minimum norm solution to a real linear least squares
            problem using a singular value decomposition (SVD)
@Purpose:   <<lss (gelss)>> computes the minimum norm solution to a real linear
            least squares problem: 
            \[
                || b - A x ||_2 \to \text{min}
            \]
            using the singular value decomposition (SVD) of $A$. $A$ is an
            $M \times N$ matrix which may be rank-deficient.
  
            Several right hand side vectors $b$ and solution vectors $x$ can
            be handled in a single call; they are stored as the columns of
            the $M \times R$ right hand side matrix $B$ and the  $N \times R$
            solution matrix $X$.

            The effective rank of $A$ is determined by  treating  as  zero
            those  singular  values which are less than RCOND times the
            largest singular value.
@Arguments:
[A]         (input/output)
            On  entry,  the $M \times N$ matrix $A$.  On exit, the first
            $\min\{m,n\}$ rows of $A$ are overwritten  with  its  right
            singular vectors, stored row-wise.
[B]         (input/output)
            On  entry,  the  $M \times R$ right hand side matrix $B$.
            
            On exit, $B$ is overwritten by the $N \times R$  solution
            matrix $X$.   If  $M \geq N$ and $\text{rank}(A) = N$, the
            residual sum-of-squares for the solution in the  $i$-th
            column is  given by the sum of squares of elements
            $B_{N+1,i}, \dots, B_{M,i}$.
@Returns:
[$i=0$]     successful exit
[$i>0$]     the algorithm for computing the SVD failed  to converge; more precisely,
            $i$ off-diagonal elements of an intermediate bi-diagonal form did not
            converge to zero.
@Todo:      \begin{enumerate}
            \item Provide a version of <<lss (gelss)>> for a single right-hand side,
                  i.\,e.\ handle the case where $B$ would be a $M \times 1$ matrix
                  (as was done for <<sv (gesv)>>).
            \item In this form the wrapper for {\tt gelss} suppresses some of the
                  output computed by its underlying LAPACK routine (e.\,g.\ the singular
                  values).
            \end{enumerate}

@@

@Key:       geev,real
@Name:      ev
@Short:     compute for an $N \times N$ real non-symmetric matrix $A$, the
            eigenvalues and, optionally, the left and/or right eigenvectors
@Purpose:   <<ev (geev,real)>> computes for an $N \times N$ real non-symmetric
            matrix $A$, the eigenvalues and, optionally, the left and/or right
            eigenvectors. The right eigenvector $v_j$ of $A$ satisfies
            \[
            A v_j = \lambda_j  v_j
            \]
            where $\lambda_j$ is its eigenvalue.
            
            The left eigenvector $u_j$ of $A$ satisfies
            \[
            u_j^H A = \lambda_j u_j^H
            \]
            where $u_j^H$ denotes the conjugate transpose of $u_j$.

            The computed eigenvectors are normalized to have Euclidean
            norm equal to $1$ and largest component real.
@Arguments:
[leftEV]    (input)
            specifies whether left eigenvectors of $A$ are computed.
[rightEV]   (input)
            specifies whether right eigenvectors of $A$ are computed.
[A]         (input/output)
            On entry, the $N \times N$ matrix $A$.
            
            On exit, $A$ has been overwritten.
[wr,wi]     (output)
            {\tt wr} and {\tt wi} contain the real and imaginary parts, respectively,
            of the computed eigenvalues. Complex conjugate pairs of eigenvalues appear
            consecutively with the eigenvalue having  the positive imaginary part first.
[vl]        (output)
            If {\tt leftEV=true}, the left eigenvectors $u_j$ are stored one after another
            in the columns of {\tt vl}, in the same order as their  eigenvalues. If
            {\tt leftEV=false}, then {\tt vl} is not referenced.
            
            If the $j$-th eigenvalue is real, then $u_j$ is stored in the $j$-th column
            of {\tt vl}. If the $j$-th and $(j+1)$-th eigenvalues form a complex
            conjugate pair, then
            \[
            u_j = \verb#vl(_,j) + i * vl(_,j+1)#
            \]
            and
            \[
            u_{j+1} = \verb#vl(_,j) - i * vl(_,j+1)#
            \]
[vr]        (output)
            If {\tt rightEV=true}, the left eigenvectors $u_j$ are stored one after another
            in the columns of {\tt vr}, in the same order as their  eigenvalues. If
            {\tt rightEV=false}, then {\tt vr} is not referenced.

            If the $j$-th eigenvalue is real, then $u_j$ is stored in the $j$-th column
            of {\tt vr}. If the $j$-th and $(j+1)$-th eigenvalues form a complex
            conjugate pair, then
            \[
            u_j = \verb#vr(_,j) + i * vr(_,j+1)#
            \]
            and
            \[
            u_{j+1} = \verb#vr(_,j) - i * vr(_,j+1)#
            \]
@Returns:   
[$i=0$]     successful exit
[$i>0$]     the $QR$ algorithm failed to compute all the eigenvalues, and no eigenvectors
            have been computed; elements $i+1$ to $N$ of {\tt wr} and  {\tt wi} contain
            eigenvalues which have converged.
@@

@Key:       geev,complex
@Name:      ev
@Short:     compute for an $N \times N$ complex non-symmetric matrix $A$, the
            eigenvalues and, optionally, the left and/or right eigenvectors
@Purpose:   <<ev (geev,complex)>> computes for an $N \times N$ complex non-symmetric
            matrix $A$, the eigenvalues and, optionally, the left and/or right
            eigenvectors. The right eigenvector $v_j$ of $A$ satisfies
            \[
            A v_j = \lambda_j  v_j
            \]
            where $\lambda_j$ is its eigenvalue.

            The left eigenvector $u_j$ of $A$ satisfies
            \[
            u_j^H A = \lambda_j u_j^H
            \]
            where $u_j^H$ denotes the conjugate transpose of $u_j$.

            The computed eigenvectors are normalized to have Euclidean
            norm equal to $1$ and largest component real.
@Arguments:
[leftEV]    (input)
            specifies whether left eigenvectors of $A$ are computed.
[rightEV]   (input)
            specifies whether right eigenvectors of $A$ are computed.
[A]         (input/output)
            On entry, the $N \times N$ matrix $A$.
            
            On exit, $A$ has been overwritten.
[w]         (output)
            contains the computed eigenvalues.
[vl]        (output)
            If {\tt leftEV=true}, the left eigenvectors $u_j$ are stored one after another
            in the columns of {\tt vl}, in the same order as their  eigenvalues. If
            {\tt leftEV=false}, then {\tt vl} is not referenced.
[vr]        (output)
            If {\tt rightEV=true}, the left eigenvectors $u_j$ are stored one after another
            in the columns of {\tt vr}, in the same order as their  eigenvalues. If
            {\tt rightEV=false}, then {\tt vr} is not referenced.
@Returns:   
[$i=0$]     successful exit
[$i>0$]     the $QR$ algorithm failed to compute all the eigenvalues, and no eigenvectors
            have been computed; elements $i+1$ to $N$ of {\tt wr} and  {\tt wi} contain
            eigenvalues which have converged.

@@

@Key:       syev
@Name:      ev
@Short:     compute all eigenvalues and, optionally, eigenvectors of a real symmetric
            matrix $A$
@Purpose:   <<ev (syev)>> computes all eigenvalues and, optionally, eigenvectors
            of a real symmetric matrix $A$.
@Arguments:
[compEV]    (input)
            specifies whether eigenvectors of $A$ are computed.
[A]         (input/output)
            On  entry,  the  symmetric matrix $A$.
            
            On successful exit and if {\tt compEV=true}, then the underlying full storage
            scheme of $A$  contains  the  orthonormal  eigenvectors  of  the matrix $A$.
            
            If {\tt compEV=false}, then  on  exit  the referenced triangle of the
            underlying full storage scheme is  destroyed.
[w]         (output)
            On successful exit, the eigenvalues in ascending order.
@Returns:
[$i=0$]     successful exit
[$i>0$]     the algorithm failed to converge; $i$ off-diagonal elements of an intermediate
            tridiagonal form did not converge to zero.

@@

@Key:       sbev
@Name:      ev
@Short:     compute all the eigenvalues and, optionally,  eigenvectors of a real symmetric
            band matrix $A$
@Purpose:   <<ev (sbev)>> computes all the eigenvalues and,  optionally,  eigenvectors of a
            real symmetric band matrix $A$.
@Arguments:
[compEV]    (input)
            specifies whether eigenvectors of $A$ are computed.
[A]         (input/output)
            On entry, the symmetric band matrix $A$.

            On exit, $A$ is overwritten by values generated  during  the  reduction  to
            tridiagonal form.
[w]         (output)
            On successful exit, the eigenvalues in ascending order.
[Z]         If {\tt compEV=true}, then on successful exit,  $Z$  contains  the
            orthonormal  eigenvectors  of the matrix $A$, with the $i$-th column of $Z$
            holding the eigenvector associated with $w(i)$.

            If {\tt compEV=false}, then $Z$ is not referenced.
@Returns:
[$i=0$]     successful exit
[$i>0$]     the algorithm failed to converge; $i$ off-diagonal elements of an intermediate
            tridiagonal form did not converge to zero.

@@

@Key:       spev
@Name:      ev
@Short:     compute all the eigenvalues and, optionally,  eigenvectors of a real symmetric
            matrix in packed storage
@Purpose:   <<ev (spev)>> computes all the eigenvalues and,  optionally,  eigenvectors of a
            real symmetric matrix in packed storage.
@Arguments:
[compEV]    (input)
            specifies whether eigenvectors of $A$ are computed.
[A]         (input/output)
            On entry, the symmetric  matrix $A$ in packed storage format.

            On exit, $A$ is overwritten by values generated during the reduction to
            tridiagonal form.
[w]         (output)
            On successful exit, the eigenvalues in ascending order.
[Z]         If {\tt compEV=true}, then on successful exit,  $Z$  contains  the
            orthonormal  eigenvectors  of the matrix $A$, with the $i$-th column of $Z$
            holding the eigenvector associated with $w(i)$.

            If {\tt compEV=false}, then $Z$ is not referenced.
@Returns:   
[$i=0$]     successful exit
[$i>0$]     the algorithm failed to converge; $i$ off-diagonal elements of an intermediate
            tridiagonal form did not converge to zero.

@@

@Key:       heev
@Name:      ev
@Short:     compute all eigenvalues and, optionally, eigenvectors of a complex Hermitian
            matrix $A$
@Purpose:   <<ev (heev)>> computes all eigenvalues and, optionally, eigenvectors of a complex
            Hermitian matrix $A$.
@Arguments:
[compEV]    (input)
            specifies whether eigenvectors of $A$ are computed.
[A]         (input/output)
            On entry, the Hermitian matrix A.
            
            On successful exit and if {\tt compEV=true}, $A$  contains  the  orthonormal
            eigenvectors of the matrix $A$.  If {\tt compEV=false}, then on exit the
            referenced triangle of the underlying full storage scheme is  destroyed.
[w]         (output)
            On successful exit, the eigenvalues in ascending order.
@Returns:   
[$i=0$]     successful exit
[$i>0$]     the algorithm failed to converge; $i$ off-diagonal elements of an intermediate
            tridiagonal form did not converge to zero.

@@

@Key:       hbev
@Name:      ev
@Short:     compute all the eigenvalues and, optionally,  eigenvectors of a complex Hermitian
            band matrix $A$
@Purpose:   <<ev (hbev)>> computes all the eigenvalues and,  optionally,  eigenvectors of a
            complex Hermitian band matrix $A$.
@Arguments:
[compEV]    (input)
            specifies whether eigenvectors of $A$ are computed.
[A]         (input/output)
            On entry, the Hermitian band matrix $A$.

            On exit, $A$ is overwritten by values generated during the reduction to
            tridiagonal form.
[w]         (output)
            On successful exit, the eigenvalues in ascending order.
[Z]         If {\tt compEV=true}, then on successful exit,  $Z$  contains  the
            orthonormal  eigenvectors  of the matrix $A$, with the $i$-th column of $Z$
            holding the eigenvector  associated with $w(i)$.
            
            If {\tt compEV=false}, then $Z$ is not referenced.
@Returns:
[$i=0$]     successful exit
[$i>0$]     the algorithm failed to converge; $i$ off-diagonal elements of an intermediate
            tridiagonal form did not converge to zero.

@@

@Key:       hpev
@Name:      ev
@Short:     compute all the eigenvalues and, optionally,  eigenvectors of a complex Hermitian
            matrix in packed storage
@Purpose:   <<ev (hpev)>> computes all the eigenvalues and,  optionally,  eigenvectors of a
            complex Hermitian matrix in packed storage.
@Arguments:
[compEV]    (input)
            specifies whether eigenvectors of $A$ are computed.
[A]         (input/output)
            On entry, the Hermitian  matrix $A$ in packed storage format.

            On exit, $A$ is overwritten by values generated during the reduction to
            tridiagonal form.
[w]         (output)
            On successful exit, the eigenvalues in ascending order.
[Z]         If {\tt compEV=true}, then on successful exit,  $Z$  contains  the
            orthonormal  eigenvectors  of the matrix $A$, with the $i$-th column of $Z$
            holding the eigenvector  associated with $w(i)$.

            If {\tt compEV=false}, then $Z$ is not referenced.
@Returns:   
[$i=0$]     successful exit
[$i>0$]     the algorithm failed to converge; $i$ off-diagonal elements of an
            intermediate tridiagonal form did not converge to zero.

@@

@Key:       gesvd
@Name:      svd
@Short:     compute the singular value decomposition (SVD) of a
            complex or real  $M \times N$  matrix $A$, optionally computing the
            left and/or right singular vectors
@Purpose:   <<svd (gesvd)>> computes the singular value decomposition (SVD) of
            a real (or complex) $M \times N$ matrix $A$, optionally computing the
            left and/or right singular vectors. The SVD is written
            \[
            A = U  \Sigma V^T \quad(\text{or}\; A = U  \Sigma V^H)
            \]
            where $\Sigma$ is an $M \times N$ matrix which is zero except for its
            $\min\{m,n\}$ diagonal  elements,  $U$ is an $M \times N$ orthogonal
            (or unitary) matrix, and $V$ is an $N \times N$ orthogonal (or unitary)
            matrix.  The  diagonal elements of $\Sigma$ are the singular values
            of $A$; they are real and non-negative, and are returned in descending
            order. The first $\min\{m,n\}$ columns of $U$ and $V$ are the left and
            right singular vectors of $A$.
            
            \noindent
            {\bf Note} that the routine returns $V^T$ (or $V^H$), not $V$.
@Arguments:
[jobu]      Specifies options for computing all or part of the matrix $U$:

            \begin{tabular}{lp{8.5cm}}
            {\tt jobu = All}      &  all $M$ columns of $U$ are returned in
                                     matrix {\tt U}\\
            {\tt jobu = SmallDim} &  the first $\min\{m,n\}$ columns of $U$
                                     (the  left singular  vectors) are returned
                                     in the matrix {\tt U}\\
            {\tt jobu = Overwrite}&  the first $\min\{m,n\}$ columns of $U$
                                     (the  left singular  vectors) are overwritten
                                     on the matrix {\tt A} \\
            {\tt jobu = None}     &  no columns of $U$ (no left singular vectors) are
                                     computed
            \end{tabular}
[jobvt]     Specifies options for computing all or part of the matrix $V^T$ (or $V^H$):

            \begin{tabular}{lp{8.5cm}}
            {\tt jobu = All}      &  all $N$ rows of $V^T$ (or $V^H$) are returned
                                     in matrix {\tt VT}\\
            {\tt jobu = SmallDim} &  the first $\min\{m,n\}$ rows of $V^T$ (or $V^H$)
                                     (the  right singular  vectors) are returned
                                     in the matrix {\tt VT}\\
            {\tt jobu = Overwrite}&  the first $\min\{m,n\}$ rows of $V^T$ (or $V^H$)
                                     (the  right singular  vectors) are overwritten
                                     on the matrix {\tt A} \\
            {\tt jobu = None}     &  no rows of $V^T$ (or $V^H$) (no right
                                     singular vectors) are computed
            \end{tabular}
            
            {\bf Note:} {\tt jobu} and {\tt jobvt} can not both be set to
            be {\tt Overwrite}.
[A]         (input/output)
            On entry, the $M \times N$ matrix $A$.
            
            On exit, if {\tt jobu = Overwrite}, $A$ is overwritten with the 
            first $\min\{m,n\}$ columns of $U$  (the  left  singular  vectors,
            stored columnwise);  if {\tt jobvt = Overwrite}, $A$ is overwritten
            with the first $\min\{m,n\}$ rows of $V^T$ (or $V^H$) (the right 
            singular vectors, stored rowwise); if {\tt jobu $\neq$ Overwrite} and
            {\tt jobu $\neq$ Overwrite}, the contents of $A$ are destroyed.
[S]         (output)
            The singular values of $A$, sorted so that $S(i) \geq S(i+1)$.
[U]         (output)
            If {\tt jobu = All}, {\tt U} contains the $M \times M$
            orthogonal (or unitary) matrix $U$; if  {\tt jobu = SmallDim},
            {\tt U}  contains  the  first $\min\{m,n\}$ columns  of $U$
            (the left singular vectors, stored columnwise); if
            {\tt jobu = None}, then {\tt U} is not referenced.
[VT]        (output)
            If {\tt jobvt = All}, {\tt VT} contains the $N \times N$
            orthogonal (or unitary) matrix $V^T$ (or $V^H$);
            if  {\tt jobu = SmallDim}, {\tt VT}  contains  the  first $\min\{m,n\}$
            rows of $V^T$ (or $V^H$) (the right singular vectors, stored rowwise);
            if {\tt jobu = None}, then {\tt VT} is not referenced.
@Returns:
[$i=0$]     successful exit
[$i>0$]     the algorithm failed to converge; $i$ specifies how many superdiagonals
            of  an intermediate bidiagonal form $B$ did not converge to zero.
